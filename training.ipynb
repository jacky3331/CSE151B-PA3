{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import model\n",
    "import datasets\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Make sure to use the GPU. The following line is just a check to see if GPU is availables\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your dataset and dataloader\n",
    "# feel free to change header of bird_dataset class\n",
    "# root = 'path to images/'\n",
    "root = 'birds_dataset/'\n",
    "train_dataset = datasets.bird_dataset(root,'train_list.txt')\n",
    "test_dataset = datasets.bird_dataset(root,'test_list.txt')\n",
    "\n",
    "# Fill in optional arguments to the dataloader as you need it\n",
    "# batch_size = 256\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create validation set?\n",
    "# train_ds, valid_ds = torch.utils.data.random_split(train_dataloader.dataset, (len(train_dataset)*0.75, len(train_dataset)*0.25))\n",
    "# # print(len(train_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e6309a2bd700>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'classification'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_dataset)):\n",
    "    sample = train_dataset[i]\n",
    "\n",
    "    print(i, sample['image'].shape, sample['classification'])\n",
    "\n",
    "    if i == 3:\n",
    "        break\n",
    "# print(train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "baseline_Net(\n",
       "  (b1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (b2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (b3): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (b4): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
       "    (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Create NN model object\n",
    "nn_model = model.baseline_Net(classes = 20)\n",
    "nn_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss functions, optimizers\n",
    "# For baseline model use this\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=1e-4)\n",
    "# Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(tensor([[[ 1.1015,  1.1016,  1.1275,  ...,  1.1178,  1.1314,  1.1248],\n         [ 1.1076,  1.1192,  1.1096,  ...,  1.1025,  1.1155,  1.1518],\n         [ 1.1187,  1.1417,  1.1428,  ...,  1.1136,  1.1350,  1.1187],\n         ...,\n         [-0.5627, -0.5894, -0.5433,  ...,  0.6847,  0.6797,  0.6508],\n         [-0.5203, -0.5646, -0.5397,  ...,  0.8398,  0.8082,  0.8082],\n         [-0.3421, -0.3603, -0.3037,  ...,  0.9378,  0.9202,  0.9202]],\n\n        [[ 1.2206,  1.2206,  1.2471,  ...,  1.2197,  1.2334,  1.2227],\n         [ 1.2082,  1.2200,  1.2102,  ...,  1.2041,  1.2163,  1.2319],\n         [ 1.1856,  1.2091,  1.2102,  ...,  1.2154,  1.2373,  1.2206],\n         ...,\n         [-0.1689, -0.2091, -0.1919,  ...,  0.7261,  0.7418,  0.7319],\n         [-0.1275, -0.1452, -0.1773,  ...,  0.8871,  0.9032,  0.9032],\n         [-0.0194, -0.0047, -0.0239,  ...,  1.0300,  1.0177,  1.0177]],\n\n        [[ 1.5245,  1.5245,  1.5510,  ...,  1.5585,  1.5721,  1.5594],\n         [ 1.5183,  1.5301,  1.5204,  ...,  1.5429,  1.5546,  1.5594],\n         [ 1.5071,  1.5305,  1.5317,  ...,  1.5542,  1.5760,  1.5594],\n         ...,\n         [ 0.3924,  0.3524,  0.4045,  ...,  1.1743,  1.1838,  1.1838],\n         [ 0.4327,  0.4043,  0.3878,  ...,  1.2906,  1.2906,  1.2906],\n         [ 0.5187,  0.5167,  0.5270,  ...,  1.3757,  1.3615,  1.3615]]]), 0)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataloader.dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1,     4] loss: 0.006\n",
      "[1,     8] loss: 0.006\n",
      "[1,    12] loss: 0.006\n",
      "[1,    16] loss: 0.006\n",
      "[1,    20] loss: 0.006\n",
      "[1,    24] loss: 0.006\n",
      "[1,    28] loss: 0.006\n",
      "[1,    32] loss: 0.006\n",
      "[1,    36] loss: 0.006\n",
      "[1,    40] loss: 0.006\n",
      "[1,    44] loss: 0.006\n",
      "[1,    48] loss: 0.006\n",
      "[1,    52] loss: 0.006\n",
      "[1,    56] loss: 0.006\n",
      "[1,    60] loss: 0.006\n",
      "[1,    64] loss: 0.006\n",
      "[1,    68] loss: 0.006\n",
      "[1,    72] loss: 0.006\n",
      "[1,    76] loss: 0.006\n",
      "[1,    80] loss: 0.006\n",
      "[1,    84] loss: 0.006\n",
      "[1,    88] loss: 0.006\n",
      "[1,    92] loss: 0.006\n",
      "[1,    96] loss: 0.006\n",
      "[1,   100] loss: 0.005\n",
      "[1,   104] loss: 0.006\n",
      "[1,   108] loss: 0.006\n",
      "[1,   112] loss: 0.006\n",
      "[1,   116] loss: 0.006\n",
      "[1,   120] loss: 0.006\n",
      "[1,   124] loss: 0.006\n",
      "[1,   128] loss: 0.005\n",
      "[1,   132] loss: 0.006\n",
      "[1,   136] loss: 0.006\n",
      "[1,   140] loss: 0.006\n",
      "[1,   144] loss: 0.006\n",
      "[1,   148] loss: 0.006\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# train your model\n",
    "# For each epoch iterate over your dataloaders/datasets, pass it to your NN model, get output, calculate loss and\n",
    "# backpropagate using optimizer\n",
    "num_epochs = 25\n",
    "loss = []\n",
    "running_loss = 0.0\n",
    "# for iteration in range(num_epochs):\n",
    "#     nn_model(train_dataloader)\n",
    "for epoch in range(1):\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = nn_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 4 == 3:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 4))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model/best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.4 64-bit",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "bd5b50cd5424d1ef650e02bcf77afc6707bf70f3a193f6eee508e2b8f8a0342a"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}